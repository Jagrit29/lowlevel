MultiThreading in JAVA

CHAPTER - 1
Process
All the apps etc that we run in our laptop those are ran by OS by creating an instance of those. This Instance is called a Process
Each Process runs completely Independant of another process and it's the job of CPU to manage these processes

Thread:
Each Process can have single or multiple threads. Threads within a process have there own stack and pointer but they share the memory and code etc.
So they are not completely independant within a process.
Thread shares the heap, code, metadata
Thread don't share the stack and the instruction pointer.

Context Switch:
Now at any time, We will have way more threads compared to the cores. So OS eventually have to context switch b/w these if we want to manage
multiple things at once. So that's where Context Switch comes into picture.
Having too many threads, It can result in thrashing where OS is spending more time context switching than actually doing anything. Context Switch
b/w threads is cheaper than b/w processes.

Thread Scheduling
Two Process - Text Editor and Music Player
Each Process have two threads one for BE and one for UI action
Now we need to decide how to schedule this, What OS does is that it divides time into epochs and then based on Dyanmic priority it assigns threads
into particular epoch
OS maintains a dynamic priority for each thread to prioritize interaction threads like UI actions and to avoid starvation of any thread.

MultiThread vs MultiProcess
Prefer MT if a lot data is shared b/w the task
MT is faster and context switch is faster
MP is better if we want better security so we can create multiple process and also if the tasks are unrelated to each other.

Why Multi Threading?
It helps us to acheive concurrency ( managing multiple tasks at once ) and parallesim ( executing multiple tasks at once ), responsiveness and high
performance
Use of multiple threads lets us manage mulitple related tasks simultaneously, which makes the app more responsive and if we execute these task
parallely, we acheive higher performance.


CHAPTER - 2
Practical of Threads

All the thread related props and methods are encapsulated by Java into a class called Thread.
We can create a thread object and we need to pass a Runnable object into that which thread class implements.
Then you can do threadObject.start() to start;
Thread class of Java also offers certain static stuffs which don't require an object and can tell you which thread you are running etc

For a particular thread, We can assign name, priority and we can set exceptionHandler for the thread so that our code doesn't break;

Thread inheritance
Another way of creating thread is extending the Thread class itself and creating a newThread class for yourself


CHAPTER - 3
Thread termination

Why and When
If thread is misbehaving, or if it's work is done, or We want to stop it because otherwise it will consume resources like
memory, cache memory etc

How
We can use thread.interrup() - if our thread has a interupt exception implemented or we need to add check like isInteruppted, etc

Daemon Threads
Background threads that don't prevent the app from exiting if the main thread ends. It's like we don't need to manually interupt these
AutoSave - This is used for background tasks like when we are saving some file and user closes it. If we don't want the file to be saved, we can just end
the app there.
We can use setDemon() to true and it won't matter whether it is complete or not and our app can end; If the thread is not demon, It will not allow
the app to end.

Thread Coordination
Controlling the threads so that we can achieve the results we want.
Different threads run independently and there order of excecution is out of our control

What if there is a thread dependency b/w 2 threads? Let's say Thread B depends on Thread A
One naive way is that whenever OS calls thread B it can check if A is complete or not, if not do nothing. now this is just wasting CPU cycles
Another way is that B thread will sleep untill thred A is complete.

For example, Let's say you have 10 big numbers you want to calc factorial of, Now naive for loop is very time extensive. What we can do is
have each thread running for each big number, This way it's faster.

Now to know if a thread is finished, We can use thread.join() which only returns when it's actually finished, means it wait for the
thread to finish its execution; Now thread join also has a limit
of waiting. If you pass 2000, It will only wait for 2sec


CHAPTER 4
Performance Optmisation
Performance metric varies from app to app, For some it's its low latency, for some accurate frame rate, For ML apps, It's the amount of data
it can ingest or the throughput.

Latency - Time to complete one task
Throughput - Amount of task completed in unit time;

Latency:
To reducing latency more, We can divide one task into multiple Independant task and run them in concurrency manner. Theoratical we can reduce
latency by N, where N is the no. of sub task but it is only possible
If the cost of breaking is free or okay, or is it even possible to break

N should be close to number of cores you have so that you can actual achieve parallesim; And all the threads should be independant ( no io blocking
 , sleep etc ) and runnable. Meaning it is always in runnable state so the core which has that thread will never leave it hence less context switch.
all the time which is nearly impossible.

We assume that CPU is dedicated for this app and it has no other things running.

Hyperthreading - Single core can run two threads parallel because the core has some duplicate items or shared things which can be used;

Inheret Cost - Breaking the tasks, Creating the threads, Starting the threads, finishing the threads, Combining the results. So small task are
not worth breaking.

So tasks can be type parallal(which can be broken), sequential, partially parallel.

Image Processing:
Each image is understood by computer as lot of pixels and each pixel has ARGB bit associated which tells it's color. Now here as the image can
be brokern down into pieces it is very useful to use multithreading if we want to cover each pixel of image to do something. Something like
coloring the image. Pixels will have coordinates like x and y. There will be improvement on multi core processors when we have big image.

If the problem can be partitioned, Multithreading can help
If no of threads are more than cores, It could be counter productive when it's pure computational task and no IO, or blocking calls; This is because
if all your tasks require CPU attention then no point. I/O, Blocking calls doesn't require that much CPU attention like making network request, etc.
There is always some inherent cost of multithreading so don't use when it's not required like for a very small image that main thread can take
care of.

Handling I/O-bound Tasks: When a thread is performing an I/O operation (like reading from a file or waiting for a network response),
it often enters a blocked state, meaning it is not using the CPU. During this time, the CPU is free to perform other tasks.
Multithreading becomes highly beneficial here because while one thread is waiting for the I/O operation to complete,
the CPU can switch to another thread that is ready to do work.

Why Can a CPU Handle I/O and Computation Together but Not Two Computations?
I/O and Computation: When handling an I/O operation, the CPU is often idle,
waiting for data to be transferred from a device or network.
During this idle time, the CPU can execute other tasks, like computations.
This parallelism between I/O and computation works well because the I/O operation doesn't require the CPU's constant attention.
Two Computations: On a single-core CPU, handling two computationally intensive tasks simultaneously doesn't provide the same benefit because both tasks require continuous CPU time.
Since the CPU can only execute one thread at a time, it must constantly switch between the two tasks.
This results in context switching overhead, which can degrade performance rather than improve it.



Throughput Improvement
Throughput is number of task completed in unit time.
It comes into picture when a system or program is flooded with concurrent flow of tasks and we want to perform as many as possible
as fast as possible

If our each tasks take T time, then our throughput is basically 1. Because in T time, We can do 1 task only.
now if we use thread and decrease the time of 1 task by which becomes T/N, then our throughput becomes N tasks or N/T. Here N is generally
the number of cores == number of threads. But this is theoricaticaly, In reality there is a inherent cost;

So to increase throughput, We should assign each task to thread. We shouldn't break the task further as that's not useful much.

Thread Pooling:
Resue of threads once it's job is done so that creation part is not duplicated. So basically ThreadPool is a pool of threads lol

Now measure Throughput w and without MT

We build a HTTP server system which will take in lot of HTTP requests and threadpool will execute each request
We can use Jmeter to do a performance test

CHAPTER 5:
Data Sharing b/w threads

Stack
Stack Memory Region - Belongs to particular thread;
Local references
Local primitive types
Stack contains args, local variables, For each method, There is a stack frame within a stack;
When a method is returned, the frame gets removed, and then the answer gets push to the older stack frame.
Stack is unique when it comes to Thread;

Heap
Heap is shared among the threads
Objects are stored on the heap.
Class Members, Static variables - That's why I declare class member like dp array on the top so that the methods can use it
Heap is managed by Garbage Collector

Object and Reference
List<Integer> list = new ArrayList<>();  Now here list is a reference and actual object is ArrayList. So that is stored in heap
but list is in stack

Why Resource Sharing b/w Threads
Resource - Something that represents data or state. For ex, variables, data structures, files, message queue or any object
Thread can share everything in heap within a process
For example, Kafka Queue or Bus is mostly a shared resource among the consumers to acheive high performance.
Database Microservice. Where multiple microservirces are hitting the db. So here db is the shared resource;

Problem of Sharing Resources in thread;
Ecommerce site. LEt's say we have inventoryCounter class which has count of items. Now if multiple threads hit this, the item variable
which is shared is gonna have problem;
Operations are not atomic in the above example; Atomic operation is all or none. Either it happened or it never happened
For example item++ is 3 step operation, First we get the value and then we do newValue = value + 1 and then we store it to value. so t
this is not atmoic if we interupt;

now item++ and items-- concurrently
As OS arranges the thread, it can result in very different results. One operation might be overrriden as it's a 3 step process.


CHAPTER 6 - Concurrency Challenges & Solution
We need to protect some area from concrurent operations so that the operation is atomic like
enter critical section
op1();
op2();
op3();
exist criticil section

Now at given time, Only 1 thread should be allowed in this critical section so that that set of operation is atomic.

Now how to guard the critical section?
Syncrhonized Keyword in JAva ( Locking mechanism)
It is used to restrict the access to given piece of code or section to single thread at a given time.

public syncronized method1()

Now this syncronized is appicable at a object level not just a method level. LEt's say if a class has 2 syncronezed method and an object
of that class is there. Now if Thread A is accessing the method 1 via object1, then no other thread can access even method2 of object1
because all the operations of that object are synchronesd; It's called monitor in JAva which locks all the synchronized method of
given object.

Second way of using a lock where we use syncronized keywrod; We need object here for the lock. Just like we can syncrhonized(this)
This is more good way as it gives us control over the kind of object we need a lock on too;

Now syncronized a reentrant meanign if Thread A is in some syncronized method, it can enter again into that. - Thread cannot prevent
itself from re-entring.

We should always minimse the critical section otherwise we would be going away from concurrrency;

Atomic Operations, Volatile & Metrics
How to know what are atomic or which not? Everything can't be made syncronzed now what to do?
So when to syncronize?
All reference assignment are atomic ( not for long or double)
Object a = new Object();
b = a;
This means our getter and setter are atomic in nature having 1 oprations. don't put 10 lines in setter and then say it's atomic
Assignments to primtive types ecvelpt long and double are atomic;
Now to make long double atomic, We need to use volatile double x = 1.0;
Java has java.util.concurrent package.
Metrics usecase -where we want to find the duration fo certain operations in production;

RACE Condition & Data Races
Race Condition - when multiple threads are accessing the same shared resource.
At least one thread is modifying the resource
The core of the problem is that we perform non atomic operations on the shared resources;
Due to race condition, our results could vary.

Now solution is to identify the race condition ( like identifying the critical section) and then making it syncronized
so that no two threads access same resource and perform unatomic operations.

Data Race:
The compiler and CPU exeucte instructions our of order to optimise the perofrmance whiling maintaining the logical correctness.
i++;
j++;

now here there could be a scenario where j will be greator than one in multithreading. That's the data race. We would think i will awlays
be greator or equal because that's placed first but no that's not true.

For example function like this is never exceuted out of order
x = 1
y = x + 2;
x = y + 10;

We can declare a shared variable as volatile which means that whenever you read or write the shared resource all the instruc are
executed before

In the above ex if I make i an y volatile
i++
j++

i will always be executed earlier.
Rule of thumb either use schronized or voltile

Locking Strategies & Deadlocks
Locking Strategy
Coarse Grain Locking - Single Lock for everything, Easy to implement but reduces concurrency.
Fine Grain Locking - Multiple Locks for different methods; This can result in deadlock
Deadlock is one where everyone wants to make progress but they are waiting on another.
For example, Let's say there are two resources protected by 2 locks
Resource A Lock A, Resource B Lock B
Now There are two threads both need Resource A and B;
Thread A gets A resource and locks it and looks for B but meanwhile B resource is locked by Thread B and it is looking for A
Now both have some things and looking for toher things which are locked by other person. So this is the case where deadlock happens

Deadlock explained by Traffic control problem of Road A and Road B

Conditions to a Deadlock;
Mutual Exclusion - Only one thread can ahve escelusive access to a resource ( One thread can access the road completely to avoid collison)
Hold and Wait - Atleast one thread is holidng a resource and waitinf ror naother;
Non preemptive allocation - Resource is only released after the thread is done using it.
Circular Wait: A chain of at least two theads each one is holding one resource and waiting for another resource

Solution to Deadlock
Avoid Circular Wait - Enforce a strict order in lock acquisition and use it everywhere;
Lock A
Lock B

Lock A
Lock B

This should be the order for any criticl section;s

There are other way like WatchDog, TryLock Operations, etc.

CHAPTER 7 - Advanced Locking
ReentrantLock - Required explict locking and unlocking; ( This is similar to using the lock on an object in syncrohnized)
In this we need to damn sure to unlock the lock. There could be a scenario where we thre an exception and it remained locked. So always
use try and finally.
It has some more methods like getOwner to tell the owner of the thead, etc etc

LockInterruptly;
Generally if we try to get a lock which is already handled by other thread, the current thread goes to sleep. Now here LockInterruptyl is
useful.

TryLock
It tries first and if it is avl then only lock but if its not avl, It gives false and moves on;


CHAPTER 8 - InterThread Communication
Sempahoes - Syncron tool, act like a permit restriction agency which can restrict n users to a paritcular resoures. Locks restrict only 1 but here semaphoreses can restrict multiple

For example, Parking lot
If a parking lot has 10 slots, We can give permit to 10 cars.

Threads acquire permits from the semaphore using sempharore.qcuire.
if the semaphore doesn't have permits, it will block the thread.

We can also think of lock is just another semaphore with permits avl 1. But there are some difference. Semaphore doesn't have notion of owner of thread
In sempahore if the same thread acrquire the sempahore and it has only can, it can result in block.
Semphaore is not good for locking.

Semphahsore - Single Produce Consumer;

acquire in semaphore is like a condition var
condition var has much more functionality
condition.await()
condition.signal or condition.signalAll()



Another ways to single - wait(), notifiy(), notifiyAll()

wait() - Causes the current thead to wait
notify() - wakes up a single thred witing on the object
notifiyAll() - wakes up all the threads waiting on that object

Here these above are appiled on the share class.